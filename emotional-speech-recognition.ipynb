{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# <center>Emotional Speech Recognition"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"7e18703a-b1a4-43af-9d09-13261b337ca3","_uuid":"a399e368-7306-4d35-992c-77760316a39e","trusted":true},"outputs":[],"source":["import os\n","\n","import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","# librosa is a Python library for analyzing audio and music.\n","# It can be used to extract the data from the audio files we will see it later\n","import librosa \n","import librosa.display\n","\n","# to play the audio files\n","from IPython.display import Audio\n","plt.style.use('seaborn-white')"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["** Since the feature extraction require a huge amount of time, before we start, in the codelines below, specify if there are already dataframes available and if so the path of those ones.  "]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["DATA_FRAMES = True\n","fem_path = '../input/features/Female_features.csv'\n","mal_path = '../input/features/Male_features.csv'"]},{"attachments":{},"cell_type":"markdown","metadata":{"_cell_guid":"a981411f-b899-458e-8733-14a260731e4a","_uuid":"c48bfdd2-3a51-4d8c-91b4-b776abb04c16","trusted":true},"source":["# <center>Emotions Speech datasets<center>  \n","\n","**Content**\n","Data set contains files from RAVDESS speechs, CREMA-D, SAVEE, TESS.\n","   \n","Out of all files data sets make up:\n","* CREMA-D - 7,442 \n","* TESS - 2,800 \n","* RAVDESS - 2,076 \n","* SAVEE - 480"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"7072a6fc-9273-4fb3-89c6-7e1e7246a484","_uuid":"142fee18-07b4-49df-a886-4ee6a3c2e3d7","trusted":true},"outputs":[],"source":["TESS = \"../input/toronto-emotional-speech-set-tess/tess toronto emotional speech set data/TESS Toronto emotional speech set data/\"\n","RAV = \"../input/ravdess-emotional-speech-audio/audio_speech_actors_01-24/\"\n","SAVEE = \"../input/surrey-audiovisual-expressed-emotion-savee/ALL/\"\n","CREMA = \"../input/cremad/AudioWAV/\""]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"a78f27ff-88d3-4220-bef3-ce16e2472153","_uuid":"1d64dee5-8878-49e3-b6b8-13fa0f528588","trusted":true},"outputs":[],"source":["# Get the data location for SAVEE\n","dir_list = os.listdir(SAVEE)\n","\n","# parse the filename to get the emotions\n","emotion=[]\n","path = []\n","for i in dir_list:\n","    if i[-8:-6]=='_a':\n","        emotion.append('angry')\n","    elif i[-8:-6]=='_d':\n","        emotion.append('disgust')\n","    elif i[-8:-6]=='_f':\n","        emotion.append('fear')\n","    elif i[-8:-6]=='_h':\n","        emotion.append('happy')\n","    elif i[-8:-6]=='_n':\n","        emotion.append('neutral')\n","    elif i[-8:-6]=='sa':\n","        emotion.append('sad')\n","    elif i[-8:-6]=='su':\n","        emotion.append('surprise')\n","    else:\n","        emotion.append('unknown') \n","    path.append(SAVEE + i)\n","\n","# Now check out the label count distribution \n","SAVEE_df = pd.DataFrame(emotion, columns = ['labels'])\n","SAVEE_df = pd.concat([SAVEE_df, pd.DataFrame(path, columns = ['path'])], axis = 1)\n","print('SAVEE dataset')\n","SAVEE_df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"7f8faee9-ebb5-4147-a013-39eb98c725c4","_uuid":"0e76fe25-cb0e-4816-b5b0-331fdce27070","trusted":true},"outputs":[],"source":["# Get the data location for TESS\n","path = []\n","emotion = []\n","dir_list = os.listdir(TESS)\n","\n","for i in dir_list:\n","    fname = os.listdir(TESS + i)   \n","    for f in fname:\n","        if i == 'OAF_angry' or i == 'YAF_angry':\n","            emotion.append('angry')\n","        elif i == 'OAF_disgust' or i == 'YAF_disgust':\n","            emotion.append('disgust')\n","        elif i == 'OAF_Fear' or i == 'YAF_fear':\n","            emotion.append('fear')\n","        elif i == 'OAF_happy' or i == 'YAF_happy':\n","            emotion.append('happy')\n","        elif i == 'OAF_neutral' or i == 'YAF_neutral':\n","            emotion.append('neutral')                                \n","        elif i == 'OAF_Pleasant_surprise' or i == 'YAF_pleasant_surprised':\n","            emotion.append('surprise')               \n","        elif i == 'OAF_Sad' or i == 'YAF_sad':\n","            emotion.append('sad')\n","        else:\n","            emotion.append('Unknown')\n","        path.append(TESS + i + \"/\" + f)\n","\n","TESS_df = pd.DataFrame(emotion, columns = ['labels'])\n","#TESS_df['source'] = 'TESS'\n","TESS_df = pd.concat([TESS_df,pd.DataFrame(path, columns = ['path'])],axis=1)\n","print('TESS dataset')\n","TESS_df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"221fd993-f8d0-4099-a9db-e82873b95a76","_uuid":"d34c2cbb-a405-4f5f-b408-7036c94c5358","trusted":true},"outputs":[],"source":["# Importing datas from RAVDESS\n","dir = os.listdir(RAV)\n","\n","males = []\n","females = [] \n","        \n","for actor in dir:\n","       \n","    files = os.listdir(RAV + actor)\n","        \n","    for file in files: \n","        part = file.split('.')[0]\n","        part = part.split(\"-\")           \n","            \n","        temp = int(part[6])        \n","                \n","        if part[2] == '01':\n","            emotion = 'neutral'\n","        elif part[2] == '02':\n","            emotion = 'calm'\n","        elif part[2] == '03':\n","            emotion = 'happy'\n","        elif part[2] == '04':\n","            emotion = 'sad'\n","        elif part[2] == '05':\n","            emotion = 'angry'\n","        elif part[2] == '06':\n","            emotion = 'fear'\n","        elif part[2] == '07':\n","            emotion = 'disgust'\n","        elif part[2] == '08':\n","            emotion = 'surprise'\n","        else:\n","            emotion = 'unknown'\n","            \n","        if temp%2 == 0:\n","            path = (RAV + actor + '/' + file)\n","            #emotion = 'female_'+emotion\n","            females.append([emotion, path]) \n","        else:\n","            path = (RAV + actor + '/' + file)\n","             #emotion = 'male_'+emotion\n","            males.append([emotion, path])   \n","    \n","   \n","RavFemales_df = pd.DataFrame(females)\n","RavFemales_df.columns = ['labels', 'path']\n","\n","RavMales_df = pd.DataFrame(males)\n","RavMales_df.columns = ['labels', 'path']\n","\n","print('RAVDESS datasets')\n","RavFemales_df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"031a3f2e-8331-47db-9853-fd6c8e7c3598","_uuid":"933a3449-7f09-40cd-b984-df2198f4c291","trusted":true},"outputs":[],"source":["RavMales_df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"e8294225-2e2b-4377-92df-b06c13413a07","_uuid":"5e4028e1-2f7c-4afc-8e9a-b817e2b0e0fc","trusted":true},"outputs":[],"source":["files = os.listdir(CREMA)\n","\n","female = [1002,1003,1004,1006,1007,1008,1009,1010,1012,1013,1018,1020,1021,1024,1025,1028,1029,1030,1037,1043,1046,1047,1049,\n","          1052,1053,1054,1055,1056,1058,1060,1061,1063,1072,1073,1074,1075,1076,1078,1079,1082,1084,1089,1091]\n","males = []\n","females = []\n","\n","for file in files: \n","    part = file.split('_')   \n","    \n","    if part[2] == 'SAD':\n","        emotion = 'sad'\n","    elif part[2] == 'ANG':\n","        emotion = 'angry'\n","    elif part[2] == 'DIS':\n","        emotion = 'disgust'\n","    elif part[2] == 'FEA':\n","        emotion = 'fear'\n","    elif part[2] == 'HAP':\n","        emotion = 'happy'\n","    elif part[2] == 'NEU':\n","        emotion = 'neutral'  \n","    else:\n","        emotion = 'unknown'\n","        \n","    if int(part[0]) in female:\n","        path = (CREMA + '/' + file)\n","        #emotion = 'female_'+emotion\n","        females.append([emotion, path]) \n","    else:\n","        path = (CREMA + '/' + file)\n","        #emotion = 'male_'+emotion\n","        males.append([emotion, path])   \n","    \n","CremaFemales_df = pd.DataFrame(females)\n","CremaFemales_df.columns = ['labels', 'path']\n","\n","CremaMales_df = pd.DataFrame(males)\n","CremaMales_df.columns = ['labels', 'path']\n","    \n","print('CREMA datasets')\n","CremaFemales_df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b196cb88-d2a2-40a5-8b8e-842803891f2b","_uuid":"4c4832b0-5fc3-4ea5-aac7-bae8daacd67d","trusted":true},"outputs":[],"source":["CremaMales_df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"92189d48-d2d0-4b9f-8457-9da943152128","_uuid":"3772939b-4f84-4c8f-94fa-dbf9b133fe71","trusted":true},"outputs":[],"source":["# Now lets merge all the dataframe\n","Males = pd.concat([SAVEE_df, RavMales_df, CremaMales_df], axis = 0)\n","Males.to_csv(\"males_emotions_df.csv\", index = False)\n","\n","Females = pd.concat([TESS_df, RavFemales_df, CremaFemales_df], axis = 0)\n","Females.to_csv(\"females_emotions_df.csv\", index = False)"]},{"attachments":{},"cell_type":"markdown","metadata":{"_cell_guid":"7c939896-00d4-4e4b-a0b3-6d6c1cf539d9","_uuid":"7421547a-5a2f-4222-bf01-2a07cf1de863","trusted":true},"source":["# <center> Data Visualization\n","\n","First, we will plot the number of emotions (of wich above there are the proportions).\n","Then using Librosa there will be some waveplots related to each emotion"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"2e0a4556-54dc-4380-9dd9-e26f0ae827bb","_uuid":"1371d315-b165-4330-b4c9-3d2729a3b982","trusted":true},"outputs":[],"source":["order = ['angry','calm','disgust','fear','happy','neutral','sad','surprise']\n","\n","fig = plt.figure(figsize=(17, 5))\n","\n","fig.add_subplot(121)\n","plt.title('Count of Females Emotions', size=16)\n","sns.countplot(Females.labels, order = order)\n","plt.ylabel('Count', size=12)\n","plt.xlabel('Emotions', size=12)\n","sns.despine(top=True, right=True, left=False, bottom=False)\n","\n","fig.add_subplot(122)\n","plt.title('Count of Males Emotions', size=16)\n","sns.countplot(Males.labels, order = order)\n","plt.ylabel('Count', size=12)\n","plt.xlabel('Emotions', size=12)\n","sns.despine(top=True, right=True, left=False, bottom=False)\n","\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"5f48d589-0add-4bb7-a8fb-f9f4a12e3c52","_uuid":"7ec88691-2fb9-4c91-8b9a-11756aefe497","trusted":true},"outputs":[],"source":["def create_waveplot(data, sr, e):\n","    plt.figure(figsize=(10, 3))\n","    plt.title(f'Waveplot for audio with {e} emotion', size=15)\n","    librosa.display.waveplot(data, sr=sr)\n","    plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"a4a58f6d-3303-4793-b896-81fcd808596f","_uuid":"1b6d7b8c-74c0-4853-b8d4-a6e7dce7576c","trusted":true},"outputs":[],"source":["emotion='Angry'\n","path = '../input/ravdess-emotional-speech-audio/Actor_01/03-01-05-01-01-01-01.wav'\n","data, sampling_rate = librosa.load(path)\n","create_waveplot(data, sampling_rate, emotion)\n","Audio(path)"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"f97d6a01-87fa-49d7-be49-c74640b7bc2b","_uuid":"34b0c3f7-6787-44c5-80bd-4c207ca8b536","trusted":true},"outputs":[],"source":["emotion='Very Angry' \n","path = '../input/ravdess-emotional-speech-audio/Actor_01/03-01-05-02-01-01-01.wav'\n","data, sampling_rate = librosa.load(path)\n","create_waveplot(data, sampling_rate, emotion)\n","Audio(path)"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"d6860263-c1df-4fd2-b0fc-77e385798849","_uuid":"139edccd-bd19-4ed8-9c61-157f411e5381","trusted":true},"outputs":[],"source":["emotion='Sing Angry'\n","path = '../input/ravdess-emotional-song-audio/Actor_01/03-02-05-01-01-01-01.wav'\n","data, sampling_rate = librosa.load(path)\n","create_waveplot(data, sampling_rate, emotion)\n","Audio(path)"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"f55752af-56ed-489c-82ad-6bd3aa4b3c6b","_uuid":"aaa11732-ab21-471e-98ea-86e993fe9aa1","trusted":true},"outputs":[],"source":["emotion='Sing Very Angry' \n","path = '../input/ravdess-emotional-song-audio/Actor_01/03-02-05-02-01-01-01.wav'\n","data, sampling_rate = librosa.load(path)\n","create_waveplot(data, sampling_rate, emotion)\n","Audio(path)"]},{"attachments":{},"cell_type":"markdown","metadata":{"_cell_guid":"bd1806f1-d8a1-4e41-a0e2-17a6c2ee41dc","_uuid":"d5d08055-1801-45f3-9a1f-a625543bb3a8","trusted":true},"source":["## Adding augmentation"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["#### Definition:\n","* Data augmentation is the process by which we create new synthetic training samples by adding small perturbations on our initial training set.\n","* The objective is to make our model invariant to those perturbations and enhace its ability to generalize.\n","* In order to this to work adding the perturbations must conserve the same label as the original training sample.\n","* In images data augmention can be performed by shifting the image, zooming, rotating ...\n","* In our case we will add noise, stretch and roll, pitch shift ..."]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b249fc24-d1f1-437d-80e6-174e8453da8f","_uuid":"7742f44f-b268-4b59-8185-36e7313d994a","trusted":true},"outputs":[],"source":["def noise(data):\n","    noise_amp = 0.04*np.random.uniform()*np.amax(data)\n","    data = data + noise_amp*np.random.normal(size=data.shape[0])\n","    return data\n","\n","def stretch(data, rate=0.70):\n","    return librosa.effects.time_stretch(data, rate)\n","\n","def shift(data):\n","    shift_range = int(np.random.uniform(low=-5, high = 5)*1000)\n","    return np.roll(data, shift_range)\n","\n","def pitch(data, sampling_rate, pitch_factor=0.8):\n","    return librosa.effects.pitch_shift(data, sampling_rate, pitch_factor)\n","\n","def higher_speed(data, speed_factor = 1.25):\n","    return librosa.effects.time_stretch(data, speed_factor)\n","\n","def lower_speed(data, speed_factor = 0.75):\n","    return librosa.effects.time_stretch(data, speed_factor)\n","\n","# taking any example and checking for techniques.\n","path = path = '../input/ravdess-emotional-speech-audio/Actor_01/03-01-05-01-01-01-01.wav'\n","data, sample_rate = librosa.load(path)"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"47835287-183e-4152-902e-cb6bb390b36c","_uuid":"0af9e19e-92bc-4262-8ca7-20307e328acd","trusted":true},"outputs":[],"source":["plt.figure(figsize=(10,3))\n","x = noise(data)\n","librosa.display.waveplot(y=x, sr=sample_rate)\n","Audio(x, rate=sample_rate)"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"867418b6-84c8-4afe-9253-912cfe11b20f","_uuid":"c9d85f0d-2d2c-4be2-9577-b597e20f4a20","trusted":true},"outputs":[],"source":["plt.figure(figsize=(10,3))\n","x = stretch(data)\n","librosa.display.waveplot(y=x, sr=sample_rate)\n","Audio(x, rate=sample_rate)"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"ecf5b48f-15a6-451a-b2b9-bacba7173fd8","_uuid":"8bb38c4d-24bd-443f-8c96-6fcccfb8f781","trusted":true},"outputs":[],"source":["plt.figure(figsize=(10,3))\n","x = shift(data)\n","librosa.display.waveplot(y=x, sr=sample_rate)\n","Audio(x, rate=sample_rate)"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"69c72492-3a27-49ce-b0e9-59a760c079a9","_uuid":"d69f44d9-2f5e-41ea-a924-f56164eb7ab0","trusted":true},"outputs":[],"source":["plt.figure(figsize=(10,3))\n","x = pitch(data, sample_rate)\n","librosa.display.waveplot(y=x, sr=sample_rate)\n","Audio(x, rate=sample_rate)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["plt.figure(figsize=(10,3))\n","x = higher_speed(data)\n","librosa.display.waveplot(y=x, sr=sample_rate)\n","Audio(x, rate=sample_rate)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["plt.figure(figsize=(10,3))\n","x = lower_speed(data)\n","librosa.display.waveplot(y=x, sr=sample_rate)\n","Audio(x, rate=sample_rate)"]},{"attachments":{},"cell_type":"markdown","metadata":{"_cell_guid":"b0f0f1bc-26cb-4e46-afe4-6a87f340d216","_uuid":"2f180b40-2a67-4f2e-bad4-70489a4e7270","trusted":true},"source":["# <center> Feature Extraction</center>"]},{"attachments":{},"cell_type":"markdown","metadata":{"_cell_guid":"34f5928d-28ba-44eb-a208-747dff0496fa","_uuid":"933364f6-33c2-4836-b004-5d8e273aa95c","trusted":true},"source":["As we understand, the data provided from audio cannot be understood by the models directly, so we need to convert them into an understandable format for which feature extraction is used.\n","The audio signal is a three-dimensional signal in which three axes represent time, amplitude and frequency.\n","\n","\n","Looking at the waveplots above seems clear (from an eye test) that the waveform itself may not necessarily yield clear class identifying information. Infact they look quite similar.<br/>  \n","It turns out one of the best tool to feature extract from audio waveforms ( and digital signal in general) is   **Mel Frequency Cepstral Coefficents (MFCCs)**.  Below we will go through a brief technical discussion, just to see how MFCCs works\n","\n","## add to references \n","* All the infos about feature extraction and audio processing were taken from https://medium.com/comet-ml/applyingmachinelearningtoaudioanalysis-utm-source-kdnuggets11-19-e160b069e88\n","* Mel Frequency Cepstral Coefficients (MFCCs), introduced by Davis and Mermelstein in 1980."]},{"attachments":{},"cell_type":"markdown","metadata":{"_cell_guid":"c409646e-553a-432b-b79f-2d4339243c62","_uuid":"2bba8bf9-d3d6-49c7-b3c6-8ca6fd55426a","trusted":true},"source":["## Mel-Frequency Cepstral Coefficients (MFCCs)\n","This feature is one of the most important method to extract a feature of an audio signal and is used majorly whenever working on audio signals. The mel frequency cepstral coefficients (MFCCs) of a signal are a small set of features (usually about 10–20) which concisely describe the overall shape of a spectral envelope."]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"495d6361-676f-4ef5-9b74-2f16e7157e61","_uuid":"588ccbb7-3456-49da-9c75-5fcf5f21cb0e","trusted":true},"outputs":[],"source":["#sample_rate = 22050\n","\n","def extract_features(data):\n","    \n","    result = np.array([])\n","    \n","    #mfccs = librosa.feature.mfcc(y=data, sr=22050, n_mfcc=42) #42 mfcc so we get frames of ~60 ms\n","    mfccs = librosa.feature.mfcc(y=data, sr=22050, n_mfcc=58)\n","    mfccs_processed = np.mean(mfccs.T,axis=0)\n","    result = np.array(mfccs_processed)\n","     \n","    return result\n","\n","def get_features(path):\n","    # duration and offset are used to take care of the no audio in start and the ending of each audio files as seen above.\n","    data, sample_rate = librosa.load(path, duration=3, offset=0.5, res_type='kaiser_fast') \n","    \n","    #without augmentation\n","    res1 = extract_features(data)\n","    result = np.array(res1)\n","    \n","    #noised\n","    noise_data = noise(data)\n","    res2 = extract_features(noise_data)\n","    result = np.vstack((result, res2)) # stacking vertically\n","    \n","    #stretched\n","    stretch_data = stretch(data)\n","    res3 = extract_features(stretch_data)\n","    result = np.vstack((result, res3))\n","    \n","    #shifted\n","    shift_data = shift(data)\n","    res4 = extract_features(shift_data)\n","    result = np.vstack((result, res4))\n","    \n","    #pitched\n","    pitch_data = pitch(data, sample_rate)\n","    res5 = extract_features(pitch_data)\n","    result = np.vstack((result, res5)) \n","    \n","    #speed up\n","    higher_speed_data = higher_speed(data)\n","    res6 = extract_features(higher_speed_data)\n","    result = np.vstack((result, res6))\n","    \n","    #speed down\n","    lower_speed_data = higher_speed(data)\n","    res7 = extract_features(lower_speed_data)\n","    result = np.vstack((result, res7))\n","    \n","    return result"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"078e399f-f2c0-4d25-b650-11d7d127bcad","_uuid":"5c82f628-b3c0-4638-ae9a-a0a5fccb7639","trusted":true},"outputs":[],"source":["if not DATA_FRAMES:\n","    \n","    female_X, female_Y = [], []\n","    for path, emotion in zip(Females.path, Females.labels):\n","        features = get_features(path)\n","        #adding augmentation, get_features return a multi dimensional array (for each augmentation), so we have to use a loop to fill the df\n","        for elem in features: \n","            female_X.append(elem)        \n","            female_Y.append(emotion)\n","    \n","\n","    male_X, male_Y = [], []\n","    for path, emotion in zip(Males.path, Males.labels):\n","        features = get_features(path)\n","        for elem in features:\n","            male_X.append(elem)\n","            male_Y.append(emotion)\n","            \n","    print(f'Check shapes:\\nFemale features: {len(female_X)}, labels: {len(female_Y)}\\nMale features:   {len(male_X)}, labels: {len(male_Y)}')"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"997a1461-28e9-431b-b90d-cf332e2f9ef3","_uuid":"463eb973-74c3-4861-8828-4ba04028613f","trusted":true},"outputs":[],"source":["def setup_dataframe(gender, features, labels):\n","    df = pd.DataFrame(features)\n","    df['labels'] = labels\n","    df.to_csv(f'{gender}_features.csv', index=False)\n","    \n","    print(f'{gender} dataframe')\n","    df.sample(frac=1).head()\n","    \n","    return df"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b18b718f-2a23-48a3-8212-aead72c1fec7","_uuid":"d2feb21f-71da-4d97-90dc-c3410cd71f5e","trusted":true},"outputs":[],"source":["if not DATA_FRAMES:\n","    Females_Features = setup_dataframe('Female', female_X, female_Y)\n","else:\n","    Females_Features = pd.read_csv(fem_path)"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"4784e5ad-2918-41b3-99fd-0904301a468f","_uuid":"ed61be7b-9002-481d-bcc4-2f6e6fe07baf","trusted":true},"outputs":[],"source":["if not DATA_FRAMES:\n","    Males_Features = setup_dataframe('Male', male_X, male_Y)\n","else:\n","    Males_Features = pd.read_csv(mal_path)"]},{"attachments":{},"cell_type":"markdown","metadata":{"_cell_guid":"a7defac7-9b53-44e4-9dcb-62efc952a60f","_uuid":"eeedb444-b208-4303-a886-6f4ac9bcd4b9","trusted":true},"source":["# <center>Data Preparation\n","As of now we have extracted the data, now we need to normalize and split our data for training and testing."]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"8b957998-6082-49eb-817a-a515aede8ca0","_uuid":"ac744cf0-b81c-4d73-b7d4-f46aad3d8dc4","trusted":true},"outputs":[],"source":["from sklearn.preprocessing import StandardScaler, OneHotEncoder\n","from sklearn.metrics import confusion_matrix, classification_report\n","from sklearn.model_selection import train_test_split"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"74df1a59-4147-48e3-a126-9864ea08ce89","_uuid":"58bb4e1a-0eeb-40b8-ae0c-f17fdfd2387e","trusted":true},"outputs":[],"source":["female_X = Females_Features.iloc[: ,:-1].values\n","female_Y = Females_Features['labels'].values\n","\n","male_X = Males_Features.iloc[: ,:-1].values\n","male_Y = Males_Features['labels'].values"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"c81ac313-e56c-4c47-8de9-47c9ce51c9ba","_uuid":"b99d9b98-3984-43b1-ad27-3058d6f4b5db","trusted":true},"outputs":[],"source":["# As this is a multiclass classification problem onehotencoding our Y.\n","encoder = OneHotEncoder()\n","\n","female_Y = encoder.fit_transform(np.array(female_Y).reshape(-1,1)).toarray()\n","male_Y = encoder.fit_transform(np.array(male_Y).reshape(-1,1)).toarray()"]},{"attachments":{},"cell_type":"markdown","metadata":{"_cell_guid":"a7492ee6-3b5b-4a4d-ac73-facf7e5d3d35","_uuid":"267c61f3-0658-46e9-bcde-889a8dff29e1","trusted":true},"source":["## Splitting data"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Just for adding more proves that gender separation have sense"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["nogender_X = np.concatenate((female_X, male_X))\n","nogender_Y = np.concatenate((female_Y, male_Y))\n","\n","x_train, x_test, y_train, y_test = train_test_split(nogender_X, nogender_Y, random_state=0, test_size=0.20, shuffle=True)\n","x_train.shape, y_train.shape, x_test.shape, y_test.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"7faf1771-a5e8-4e39-aff7-9177bfdb7eab","_uuid":"ca782f6d-060b-42b4-ab37-0ace11841509","trusted":true},"outputs":[],"source":["x_trainF, x_testF, y_trainF, y_testF = train_test_split(female_X, female_Y, random_state=0, test_size=0.20, shuffle=True)\n","x_trainF.shape, y_trainF.shape, x_testF.shape, y_testF.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"989d4298-07cf-4875-81c8-f64950eb598a","_uuid":"094de711-d89e-49ab-8afa-57bb9a2856ab","trusted":true},"outputs":[],"source":["x_trainM, x_testM, y_trainM, y_testM = train_test_split(male_X, male_Y, random_state=0, test_size=0.20, shuffle=True)\n","x_trainM.shape, y_trainM.shape, x_testM.shape, y_testM.shape"]},{"attachments":{},"cell_type":"markdown","metadata":{"_cell_guid":"cd518403-6a48-4430-b95c-7597b8708d8e","_uuid":"87cbc221-1eed-48b5-8113-e6764762485e","trusted":true},"source":["* https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html  \n","We are going to scale our features throught the StandarScaler module, it standardize the features in a **Normal curve**, i.e.:<br><br>\n","  <center> $Z = (X -{\\mu})/{\\sigma}$. </center><br>\n","*Standardization of a dataset is a common requirement for many machine learning estimators: they might behave badly if the individual features do not more or less look like standard normally distributed data (e.g. Gaussian with 0 mean and unit variance).*"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"f4640e2b-eaed-41d7-81b4-ba65bb8e0486","_uuid":"0ee2bea2-7ee3-43ce-ae2a-cd4a28f1c41f","trusted":true},"outputs":[],"source":["scaler = StandardScaler()\n","\n","x_train = scaler.fit_transform(x_train)\n","x_test = scaler.transform(x_test)\n","\n","x_trainF = scaler.fit_transform(x_trainF)\n","x_testF = scaler.transform(x_testF)\n","\n","x_trainM = scaler.fit_transform(x_trainM)\n","x_testM = scaler.transform(x_testM)"]},{"attachments":{},"cell_type":"markdown","metadata":{"_cell_guid":"365d8795-2307-48cf-b01c-a4028253126b","_uuid":"254dc31b-fb2b-4af2-b9e7-e3c0e9c0153e","trusted":true},"source":["## Making our data compatible to model"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["x_train = np.expand_dims(x_train, axis=2)\n","x_test = np.expand_dims(x_test, axis=2)\n","x_train.shape, y_train.shape , x_test.shape , y_test.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"c22879c0-9f9f-4895-8ec2-2b5a5041842a","_uuid":"7b6187b7-751b-4f3f-91ed-c114778cfc44","trusted":true},"outputs":[],"source":["x_trainF = np.expand_dims(x_trainF, axis=2)\n","x_testF = np.expand_dims(x_testF, axis=2)\n","x_trainF.shape, y_trainF.shape, x_testF.shape, y_testF.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"786a150e-181d-45d3-972d-1c688adcdb6d","_uuid":"5fc3272f-e82f-4645-83bd-917876decca5","trusted":true},"outputs":[],"source":["x_trainM = np.expand_dims(x_trainM, axis=2)\n","x_testM = np.expand_dims(x_testM, axis=2)\n","x_trainM.shape, y_trainM.shape, x_testM.shape, y_testM.shape"]},{"attachments":{},"cell_type":"markdown","metadata":{"_cell_guid":"c9d8ce9e-3075-4a91-9fa2-bb6f468b3ab9","_uuid":"3a24a0e5-62b8-47a8-8623-fdb122fbb967","trusted":true},"source":["# <center>Modeling<center>"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"3784657e-a214-4643-9b37-c1cdc0d779b5","_uuid":"276d227e-ccc8-47a8-b7f0-e1f9af5b01f8","trusted":true},"outputs":[],"source":["import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras import layers\n","\n","from keras.callbacks import ReduceLROnPlateau\n","from keras.models import Sequential\n","from keras.layers import Dense, Conv1D, MaxPooling1D, Flatten, Dropout, BatchNormalization, AveragePooling1D\n","from keras.utils import np_utils, to_categorical\n","from keras.callbacks import ModelCheckpoint"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"617f366b-db5a-4a5f-b2ec-c989d92220c7","_uuid":"02667e51-976b-45be-9424-c6031256dc5b","trusted":true},"outputs":[],"source":["print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"03735553-216e-4a5d-932d-a7084ec7c3d3","_uuid":"0f302020-e0cc-4583-8396-b65e53f9bdbc","trusted":true},"outputs":[],"source":["# Create a MirroredStrategy.\n","strategy = tf.distribute.MirroredStrategy()\n","print('Number of devices: {}'.format(strategy.num_replicas_in_sync))"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"025457f6-da9c-49c5-820c-0258547185cb","_uuid":"3ed66eef-3c8d-40f6-bf6c-2dae3f074d7e","trusted":true},"outputs":[],"source":["with strategy.scope():\n","    \n","    def build_model(in_shape):\n","        \n","        model=Sequential()\n","        model.add(Conv1D(256, kernel_size=6, strides=1, padding='same', activation='relu', input_shape=(in_shape, 1)))\n","        model.add(AveragePooling1D(pool_size=4, strides = 2, padding = 'same'))\n","\n","        model.add(Conv1D(128, kernel_size=6, strides=1, padding='same', activation='relu'))\n","        model.add(AveragePooling1D(pool_size=4, strides = 2, padding = 'same'))\n","\n","        model.add(Conv1D(128, kernel_size=6, strides=1, padding='same', activation='relu'))\n","        model.add(AveragePooling1D(pool_size=4, strides = 2, padding = 'same'))\n","        model.add(Dropout(0.2))\n","\n","        model.add(Conv1D(64, kernel_size=6, strides=1, padding='same', activation='relu'))\n","        model.add(MaxPooling1D(pool_size=4, strides = 2, padding = 'same'))\n","        \n","        model.add(Flatten())\n","        model.add(Dense(units=32, activation='relu'))\n","        model.add(Dropout(0.3))\n","\n","        model.add(Dense(units=8, activation='softmax'))\n","        model.compile(optimizer = 'adam' , loss = 'categorical_crossentropy' , metrics = ['accuracy'])\n","          \n","        \n","        return model"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"0669ff45-5978-45bb-aa26-2e8f419f98e7","_uuid":"ca817c54-848e-4bc3-8182-48f792a3a616","trusted":true},"outputs":[],"source":["def model_build_summary(mod_dim, tr_features, val_features, val_labels):\n","    model = build_model(mod_dim)\n","    model.summary()\n","    \n","    score = model.evaluate(val_features, val_labels, verbose = 1)\n","    accuracy = 100*score[1]\n","    \n","    return model"]},{"attachments":{},"cell_type":"markdown","metadata":{"_cell_guid":"8df4fe0c-88ee-4170-9835-188a15b06574","_uuid":"e4fcfecf-a921-45ee-a37a-ac8674d790e4","trusted":true},"source":["*ReduceLROnPlateau* reduce learning rate when a metric has stopped improving.<br>\n","\n","Models often benefit from reducing the learning rate by a factor of 2-10 once learning stagnates. This callback monitors a quantity and if no improvement is seen for a 'patience' number of epochs, the learning rate is reduced."]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"6f0e6211-dcfe-4ebc-abe8-57ad3d31c0fb","_uuid":"6f9617eb-6955-4264-86ff-16de5fa2b4ff","trusted":true},"outputs":[],"source":["rlrp = ReduceLROnPlateau(monitor='loss', factor=0.4, verbose=0, patience=4, min_lr=0.000001)\n","\n","batch_size = 32\n","n_epochs = 75"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b9dbc777-4949-4982-8534-48dc46cc6dcd","_uuid":"657294c0-b559-421f-9f50-3ea13b1ed691","trusted":true},"outputs":[],"source":["def show_graphs(history):\n","    epochs = [i for i in range(n_epochs)]\n","    fig , ax = plt.subplots(1,2)\n","    train_acc = history.history['accuracy']\n","    train_loss = history.history['loss']\n","    test_acc = history.history['val_accuracy']\n","    test_loss = history.history['val_loss']\n","\n","    fig.set_size_inches(30,12)\n","    ax[0].plot(epochs , train_loss , label = 'Training Loss')\n","    ax[0].plot(epochs , test_loss , label = 'Testing Loss')\n","    ax[0].set_title('Training & Testing Loss')\n","    ax[0].legend()\n","    ax[0].set_xlabel(\"Epochs\")\n","\n","    ax[1].plot(epochs , train_acc , label = 'Training Accuracy')\n","    ax[1].plot(epochs , test_acc , label = 'Testing Accuracy')\n","    ax[1].set_title('Training & Testing Accuracy')\n","    ax[1].legend()\n","    ax[1].set_xlabel(\"Epochs\")\n","    plt.show()"]},{"attachments":{},"cell_type":"markdown","metadata":{"_cell_guid":"d0162cd2-734a-45cf-bd6f-1d33037feb36","_uuid":"b8db12a0-b5f5-4831-ae7c-ce3b80b64f30","trusted":true},"source":["## Model Summary and Pre-training Accuracy"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["total_model = model_build_summary(x_train.shape[1], x_train, x_test, y_test)"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"853d7c4b-3942-4e2e-92c8-6f1416d07bfd","_uuid":"fbc003e4-b7f4-4fd6-9540-67adf0a11d6a","trusted":true},"outputs":[],"source":["female_model = model_build_summary(x_trainF.shape[1], x_trainF, x_testF, y_testF)"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"dc2e6c56-87b3-4830-aa1d-2bb0ff9b4e9d","_uuid":"f4236892-65fe-43c6-a430-66fd9ce42fd3","trusted":true},"outputs":[],"source":["male_model = model_build_summary(x_trainM.shape[1], x_trainM, x_testM, y_testM)"]},{"attachments":{},"cell_type":"markdown","metadata":{"_cell_guid":"e2b79fb2-daf8-4a17-a012-a37263f9a8d0","_uuid":"f1d4a537-a729-47ea-b935-4fe454580ecf","trusted":true},"source":["## Model training"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["history = total_model.fit(x_train, y_train, batch_size=batch_size, epochs=n_epochs, validation_data=(x_test, y_test), callbacks=[rlrp])"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"683e39d7-fb4e-4c17-9e95-02de1ca51cd5","_uuid":"dff9b1bf-1325-4ba2-936a-78cca34c36fb","trusted":true},"outputs":[],"source":["female_history = female_model.fit(x_trainF, y_trainF, batch_size=batch_size, epochs=n_epochs, validation_data=(x_testF, y_testF), callbacks=[rlrp])"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"f731dbe4-7c4e-4dce-9d53-1acfdaa2b024","_uuid":"bf5377cb-11fe-458a-888e-dc9bdbcd2784","trusted":true},"outputs":[],"source":["male_history = male_model.fit(x_trainM, y_trainM, batch_size=batch_size, epochs=n_epochs, validation_data=(x_testM, y_testM), callbacks=[rlrp])"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["**Uncomment the code below to see the output of a specific layer"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["'''\n","from keras import backend as K\n","\n","layer_name = 'conv1d_11'\n","intermediate_layer_model = keras.Model(inputs=female_model.input,\n","                                       outputs=female_model.get_layer(layer_name).output)\n","intermediate_output = intermediate_layer_model(x_testF)\n","print(intermediate_output[1,0])\n","'''"]},{"attachments":{},"cell_type":"markdown","metadata":{"_cell_guid":"dfe9a30b-84ad-4628-a18b-c3db6af78163","_uuid":"2b58f999-e771-492f-b28a-ca7d010f73ab","trusted":true},"source":["## Performance Evaluations"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# genderless\n","score = total_model.evaluate(x_train,y_train, verbose = 0)\n","print(\"Mixed-gender emotions training Accuracy: {0:.2%}\".format(score[1]))\n","\n","score = total_model.evaluate(x_test, y_test, verbose=0)\n","print(\"Mixed-gender emotions testing Accuracy: {0:.2%}\".format(score[1]))"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b3be869d-46c0-4d0b-b093-6ada4af95417","_uuid":"2ed7d9cb-4602-4f7a-80d2-05cab4d6abb4","trusted":true},"outputs":[],"source":["score = female_model.evaluate(x_trainF,y_trainF, verbose = 0)\n","print(\"Female emotions training Accuracy: {0:.2%}\".format(score[1]))\n","\n","score = female_model.evaluate(x_testF, y_testF, verbose=0)\n","print(\"Female emotions testing Accuracy: {0:.2%}\".format(score[1]))"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"2efe6f55-a21c-4ae1-8c2f-eda2c9564469","_uuid":"5b0462f9-1f5b-4908-a3f0-315477dd0176","trusted":true},"outputs":[],"source":["score = male_model.evaluate(x_trainM,y_trainM, verbose = 0)\n","print(\"Male emotions training Accuracy: {0:.2%}\".format(score[1]))\n","\n","score = male_model.evaluate(x_testM, y_testM, verbose=0)\n","print(\"Male emotions testing Accuracy: {0:.2%}\".format(score[1]))"]},{"attachments":{},"cell_type":"markdown","metadata":{"_cell_guid":"2ae8241f-ac42-4312-ad18-2643b39f2ae8","_uuid":"221e07a6-f3cb-41fb-a2c4-52536a15b8c7","trusted":true},"source":["## Training and Validation trends"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["show_graphs(history)"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"16efab44-61a0-4393-87e6-e8d379f853d7","_uuid":"be327c88-e087-4eb8-a928-a29c60f8633d","trusted":true},"outputs":[],"source":["show_graphs(female_history)"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"26b8f452-bce7-422f-a988-16de7685da76","_uuid":"72f72742-ccc6-480a-88f0-97f8477639fb","trusted":true},"outputs":[],"source":["show_graphs(male_history)"]},{"attachments":{},"cell_type":"markdown","metadata":{"_cell_guid":"ce5b7034-fbdc-4ba5-be4f-ca91023e53bf","_uuid":"aaa74df4-a2dd-460a-85d1-93dede24d13c","trusted":true},"source":["## Confusion matrix"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"4984e605-6e49-460c-a9e1-4042abc00f78","_uuid":"26fea659-f7ad-47eb-8e04-119ff1169329","trusted":true},"outputs":[],"source":["# predicting on test data.\n","pred_test = female_model.predict(x_testF)\n","y_pred = encoder.inverse_transform(pred_test)\n","y_test_ = encoder.inverse_transform(y_testF)"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"08b69691-4a7e-4c71-961b-ab16c020eb79","_uuid":"f083eaea-21b0-49a3-bea9-e693e7a77453","trusted":true},"outputs":[],"source":["cm = confusion_matrix(y_test_, y_pred)\n","plt.figure(figsize = (12, 10))\n","cm = pd.DataFrame(cm , index = [i for i in encoder.categories_] , columns = [i for i in encoder.categories_])\n","sns.heatmap(cm, linecolor='white', cmap='Blues', linewidth=1, annot=True, fmt='')\n","plt.title('Confusion Matrix for Female Emotions', size=20)\n","plt.xlabel('Predicted Labels', size=14)\n","plt.ylabel('Actual Labels', size=14)\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"27a1522a-d7f3-40f3-a621-e6aadb093de7","_uuid":"a9afc111-20f1-4a9b-b302-f3fdd0d1e1a7","trusted":true},"outputs":[],"source":["# predicting on test data.\n","pred_test = male_model.predict(x_testM)\n","y_pred = encoder.inverse_transform(pred_test)\n","y_test_ = encoder.inverse_transform(y_testM)"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"678e9d91-862a-4159-9877-caa275302c8e","_uuid":"e45908f7-d67f-4442-92cd-11d50f224aa6","trusted":true},"outputs":[],"source":["cm = confusion_matrix(y_test_, y_pred)\n","plt.figure(figsize = (12, 10))\n","cm = pd.DataFrame(cm , index = [i for i in encoder.categories_] , columns = [i for i in encoder.categories_])\n","sns.heatmap(cm, linecolor='white', cmap='Blues', linewidth=1, annot=True, fmt='')\n","plt.title('Confusion Matrix for Male Emotions', size=20)\n","plt.xlabel('Predicted Labels', size=14)\n","plt.ylabel('Actual Labels', size=14)\n","plt.show()"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.4"}},"nbformat":4,"nbformat_minor":4}
